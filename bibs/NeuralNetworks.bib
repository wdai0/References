@article{arfeStochasticApproximationSA,
  title = {Stochastic {{Approximation}} ({{SA}}) and {{Martingale}} Methods},
  author = {Arfe, Andrea},
  langid = {english},
  file = {/Users/wei/Zotero/storage/5AZ92Y4Z/Arfe - Stochastic Approximation (SA) and Martingale metho.pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12},
  journaltitle = {Mathematics of Control, Signals, and Systems},
  shortjournal = {Math. Control Signal Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  url = {http://link.springer.com/10.1007/BF02551274},
  urldate = {2023-06-15},
  langid = {english},
  annotation = {8213 citations (Crossref) [2023-06-14]},
  file = {/Users/wei/Zotero/storage/7VTKNUYQ/Cybenko_1989_Approximation by superpositions of a sigmoidal function.pdf}
}

@article{luExpressivePowerNeural,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  langid = {english},
  file = {/Users/wei/Zotero/storage/TDYVDIRQ/Lu et al. - The Expressive Power of Neural Networks A View fr.pdf}
}

@incollection{robbinsCONVERGENCETHEOREMNON1971,
  title = {A {{CONVERGENCE THEOREM FOR NON NEGATIVE ALMOST SUPERMARTINGALES AND SOME APPLICATIONS}}**{{Research}} Supported by {{NIH Grant}} 5-{{R01-GM-16895-03}} and {{ONR Grant N00014-67-A-0108-0018}}.},
  booktitle = {Optimizing {{Methods}} in {{Statistics}}},
  author = {Robbins, H. and Siegmund, D.},
  date = {1971},
  pages = {233--257},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-604550-5.50015-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780126045505500158},
  urldate = {2023-06-09},
  isbn = {978-0-12-604550-5},
  langid = {english}
}

@article{schmidt-hieberNonparametricRegressionUsing2020,
  title = {Nonparametric Regression Using Deep Neural Networks with {{ReLU}} Activation Function},
  author = {Schmidt-Hieber, Johannes},
  date = {2020-08-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {48},
  number = {4},
  issn = {0090-5364},
  doi = {10.1214/19-AOS1875},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-4/Nonparametric-regression-using-deep-neural-networks-with-ReLU-activation-function/10.1214/19-AOS1875.full},
  urldate = {2023-06-09},
  langid = {english},
  annotation = {67 citations (Crossref) [2023-06-14]},
  file = {/Users/wei/Zotero/storage/CHXARRC4/Schmidt-Hieber - 2020 - Nonparametric regression using deep neural network.pdf}
}

@article{zhongDeepLearningPartially2022,
  title = {Deep Learning for the Partially Linear {{Cox}} Model},
  author = {Zhong, Qixian and Mueller, Jonas and Wang, Jane-Ling},
  date = {2022-06-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {50},
  number = {3},
  issn = {0090-5364},
  doi = {10.1214/21-AOS2153},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-3/Deep-learning-for-the-partially-linear-Cox-model/10.1214/21-AOS2153.full},
  urldate = {2023-06-09},
  langid = {english},
  annotation = {2 citations (Crossref) [2023-06-14]},
  file = {/Users/wei/Zotero/storage/I9AMMUYA/Zhong et al. - 2022 - Deep learning for the partially linear Cox model.pdf}
}
