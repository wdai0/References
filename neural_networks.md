**Date:** “Last compiled on 14 June, 2023”

Useful links and references about universal approximation theory:

1.  [Understanding deep
    learning](https://udlbook.github.io/udlbook/)(draft).

2.  [Matus Telgarsky’s notes](https://mjt.cs.illinois.edu/dlt/two.pdf)

3.  [M. Telgarsky, Benefits of depth in neural
    networks](http://proceedings.mlr.press/v49/telgarsky16.pdf)

4.  [Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of
    neural networks: A view from the
    width.](https://proceedings.neurips.cc/paper_files/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf)
    also, \[1\].

5.  [G. Cybenko, Approximation by superpositions of a sigmoidal
    function.](https://link.springer.com/article/10.1007/bf02551274),
    \[2\]

<div id="refs" class="references csl-bib-body">

<div id="ref-luExpressivePowerNeural" class="csl-entry">

<span class="csl-left-margin">\[1\] </span><span
class="csl-right-inline">Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang, “The
Expressive Power of Neural Networks: A View from the Width.”</span>

</div>

<div id="ref-cybenkoApproximationSuperpositionsSigmoidal1989"
class="csl-entry">

<span class="csl-left-margin">\[2\] </span><span
class="csl-right-inline">G. Cybenko, “Approximation by superpositions of
a sigmoidal function,” *Math. Control Signal Systems*, vol. 2, no. 4,
pp. 303–314, Dec. 1989, doi:
[10.1007/BF02551274](https://doi.org/10.1007/BF02551274).</span>

</div>

<div id="ref-arfeStochasticApproximationSA" class="csl-entry">

<span class="csl-left-margin">\[3\] </span><span
class="csl-right-inline">A. Arfe, “Stochastic Approximation (SA) and
Martingale methods.”</span>

</div>

<div id="ref-robbinsCONVERGENCETHEOREMNON1971" class="csl-entry">

<span class="csl-left-margin">\[4\] </span><span
class="csl-right-inline">H. Robbins and D. Siegmund, “A CONVERGENCE
THEOREM FOR NON NEGATIVE ALMOST SUPERMARTINGALES AND SOME
APPLICATIONS\*\*Research supported by NIH Grant 5-R01-GM-16895-03 and
ONR Grant N00014-67-A-0108-0018.” in *Optimizing Methods in Statistics*,
Elsevier, 1971, pp. 233–257. doi:
[10.1016/B978-0-12-604550-5.50015-8](https://doi.org/10.1016/B978-0-12-604550-5.50015-8).</span>

</div>

<div id="ref-schmidt-hieberNonparametricRegressionUsing2020"
class="csl-entry">

<span class="csl-left-margin">\[5\] </span><span
class="csl-right-inline">J. Schmidt-Hieber, “Nonparametric regression
using deep neural networks with ReLU activation function,” *Ann.
Statist.*, vol. 48, no. 4, Aug. 2020, doi:
[10.1214/19-AOS1875](https://doi.org/10.1214/19-AOS1875).</span>

</div>

<div id="ref-zhongDeepLearningPartially2022" class="csl-entry">

<span class="csl-left-margin">\[6\] </span><span
class="csl-right-inline">Q. Zhong, J. Mueller, and J.-L. Wang, “Deep
learning for the partially linear Cox model,” *Ann. Statist.*, vol. 50,
no. 3, Jun. 2022, doi:
[10.1214/21-AOS2153](https://doi.org/10.1214/21-AOS2153).</span>

</div>

</div>
